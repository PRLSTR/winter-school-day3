# winter-school-day3

В обох випадках був використаний метод Q-Learning. Це алгоритм підкріплювального навчання, що навчає агента вибирати найкращі варіанти дій в різних ситуаціях. Агент коли взаємодіє із середовищем, отримує винагороду, на основі якої оцінює дію. Ці оцінки зберігаються у таблиці, де певне значення показує наскільки дія хороша чи погана у певній ситуації. Сам Gymnasium має кілька вже наперед готових середовищ, щоб навчати агента.
Першим середовищем було обрано Taxi. У цьому середовищі агент керує таксі і йому потрібно під'їхати до пасажира, забрати його та відвести у потрібну точку. За зайві кроки агент отримує штраф, а за успішну доставку навпаки нагороду.
На цьому відео видно, що на першій спробі агент довго не підбирав пасажира і навіть після цього він неодноразово його висаджував і знову підбирав, отримуючи штрафи. Після 2000 спроб виконати задачу від початку до кінця, агент почав справлятися із задачею, роблячи необхідний мінімум кроків для виконання доставки пасажира.

https://github.com/user-attachments/assets/b7514c92-a10e-4be5-a24c-b131227d8d9e

На цьому графіку можна побачити, що агент на перших спробах отримував майже -400 балів, але вже ближче до 400 спроби почав отримувати більше 0 балів, оскільки агент покращив свою стратегію по виконанню цієї задачі і вже аж до кінця притримувався її.

<img width="1000" height="500" alt="taxi_q-learning_metrics" src="https://github.com/user-attachments/assets/c7082044-401a-465e-8833-a0c468207e34" />

Другим середовищем було обрано Cliff Walking. В цьому середовищі агент повинен дійти від старту до фінішу вздовж обриву. Якщо він зайде на його клітинки, то отримає великий штраф та повернеться до початку. Відповідно ціль у цьому середовищі - знайти найкоротший та безпечний шлях до фінішу.
На відео видно, що на першій спробі агент більшу частину часу заходив на клітинки обриву, тим самим повертаючись до початку навіть не вибравшись звідти. Як результат було отримано -1090 балів. Після 2000 спроб, агент також почав справлятись із цією задачею.

https://github.com/user-attachments/assets/f30490c3-c015-4870-ad4f-be7073ef841f

На цьому графіку видно, що агент знайшов правильну стратегію досить таки швидко, для цього знадобилося менш ніж 250 спроб.

<img width="1000" height="500" alt="cliff_q-learning_metrics" src="https://github.com/user-attachments/assets/b5f396e4-78f1-4be9-b4f9-86f440ccd8d3" />

